from config import *
from generate_prompt import *
'''
**è¯¥æ–‡ä»¶å†…å­˜å‚¨å®Œæˆå„ç§åŸºæœ¬æ“ä½œçš„å‡½æ•°**

åŒ…æ‹¬ï¼š

get_doc(function_name)ï¼šæ ¹æ®å‡½æ•°åè·å–å‡½æ•°çš„æ–‡æ¡£å­—ç¬¦ä¸²

extract_parameters_torch(api_doc)ï¼šæ ¹æ®torchå‡½æ•°æ–‡æ¡£è·å–å‚æ•°åˆ—è¡¨

extract_parameters_tf(api_doc)ï¼šæ ¹æ®tfå‡½æ•°æ–‡æ¡£è·å–å‚æ•°åˆ—è¡¨

generate_all_combinations(args)ï¼šè·å–æ‰€æœ‰å‚æ•°çš„ç»„åˆ
 
filter_combinations(combinations, condition)ï¼šè¿‡æ»¤ä¸åˆæ³•çš„å‚æ•°ç»„åˆ

read_file(file_path)ï¼šè¯»å–æ–‡ä»¶

append_api_condition_to_json(fun_string, file_path, new_doc_str)ï¼šå‘JSONæ–‡ä»¶ä¸­æ·»åŠ APIæ¡ä»¶

get_api_conditions(fun_string, file_path)ï¼šè·å–JSONæ–‡ä»¶ä¸­çš„api_conditions

append_filtered_combinations_to_json(path, fun_string, new_data)ï¼šå‘JSONæ–‡ä»¶ä¸­æ·»åŠ è¿‡æ»¤åçš„å‚æ•°ç»„åˆ

add_log(log)ï¼šæ‰“å°æ—¥å¿—åˆ°æ§åˆ¶å°å’Œæ–‡ä»¶
'''



def filter_apidocument(api_doc):
    # å®šä¹‰æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼ï¼ŒåŒ¹é…See :class:`~åˆ°` for more details.ä¹‹é—´çš„å†…å®¹
    if lib_name == "torch":
        pattern_0 = r':class:`~(.*?)` for more'
        match_0 = re.search(pattern_0, api_doc)

        pattern_1 = r'See :class:`~(.*?)`'
        match_1 = re.search(pattern_1, api_doc)

        pattern_2 = r'See :class:`(.*?)` for details'
        match_2 = re.search(pattern_2, api_doc)

        pattern_3 = r'Alias of :func:`(.*?)`'
        match_3 = re.search(pattern_3, api_doc)

        pattern_4 = r'of :meth:`(.*?)`'
        match_4 = re.search(pattern_4, api_doc)

        pattern_5 = r'Alias for :func:`(.*?)`'
        match_5 = re.search(pattern_5, api_doc)

        if match_0:
            return match_0.group(1)  # è¿”å›æ•è·ç»„ä¸­çš„å†…å®¹
        elif match_1:
            return match_1.group(1)
        elif match_2:
            return match_2.group(1)
        elif match_3:
            return match_3.group(1)
        elif match_4:
            return match_4.group(1)
        elif match_5:
            return match_5.group(1)
        return None  # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ï¼Œè¿”å›None
    elif lib_name == "tf":
        pattern_0 = r':class:`~(.*?)` for more'
        match_0 = re.search(pattern_0, api_doc)



#æ ¹æ®å‡½æ•°åè·å–å‡½æ•°çš„æ–‡æ¡£å­—ç¬¦ä¸²
def get_doc(function_name):
    
    if lib_name == "torch":
        if function_name in torch_samename_data:
            return torch_samename_data[function_name]
        
        if function_name in ["torch.scatter", "torch.scatter_add"]:
            return eval(filter_apidocument(eval(function_name).__doc__)).__doc__
        if function_name.endswith("_"):
            function_name_ = function_name[:-1]
            try:
                function = eval(function_name)
                api_doc_1 = function.__doc__
            except (AttributeError, ImportError, NameError) as e:
                return False
            try:
                function = eval(function_name_)
                api_doc_2 = function.__doc__
            except (AttributeError, ImportError, NameError) as e:
                return False
            
            return api_doc_1 + '\n' + api_doc_2 + '\n' + get_doc(function_name_)
        
        try:
            function = eval(function_name)
            api_doc = function.__doc__
            if api_doc is None:
                return False
    
        except (AttributeError, ImportError, NameError) as e:
            return False
        hash_list = ["Args:" in api_doc , 
                     "math::" in api_doc, 
                     "Shape:" in api_doc , 
                     "Arguments:" in api_doc , 
                     "-> torch.dtype" in api_doc , 
                     "from_numpy(ndarray)" in api_doc , 
                     "torch.moveaxis" in api_doc , 
                     "Examples:" in api_doc ,
                     function_name == "torch.seed",
                     function_name =="torch.initial_seed",
                     function_name =="torch.get_rng_state",
                     function_name =="torch.get_num_threads",
                     function_name =="torch.get_num_interop_threads",
                     function_name =="torch.compiled_with_cxx11_abi",
                     function_name =="torch.are_deterministic_algorithms_enabled"
                     ]

        if True in hash_list:
            return api_doc
        else:
            func_name = filter_apidocument(api_doc)
            if func_name is None:
                return api_doc
            return get_doc(func_name)          
        
    elif lib_name == "tf":
        try:
            function = eval(function_name)
            api_doc = function.__doc__
        except (AttributeError, ImportError, NameError) as e:
            return False

        if api_doc is None:
            return False
        
        return api_doc


#æ ¹æ®å‡½æ•°æ–‡æ¡£è·å–å‚æ•°åˆ—è¡¨
#é’ˆå¯¹torchå‡½æ•°æ–‡æ¡£è¿›è¡Œå¤„ç†
def extract_parameters_torch(api_doc, api_def):
    
    if len(api_doc) > len(api_def):
        new_api_doc = api_doc[:len(api_def)+100]
    else:
        new_api_doc = api_doc
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…ç¬¬ä¸€ä¸ªæ‹¬å·å†…çš„å†…å®¹ï¼ˆå‚æ•°éƒ¨åˆ†ï¼‰
    match = re.search(r'\((.*?)\)', new_api_doc)
    
    if not match:
        match_1 = re.search(r'\((.*?)\)', api_def)
        param_str = match_1.group(1)
        # å¤„ç†å‚æ•°å­—ç¬¦ä¸²
        parameters = [p.strip().split('=')[0] for p in param_str.split(',')]
        for i in parameters:
            if i == '*':
                parameters.remove(i)
        return parameters
    else:
        param_str = match.group(1)
        # å¤„ç†å‚æ•°å­—ç¬¦ä¸²
        parameters = [p.strip().split('=')[0] for p in param_str.split(',')]
        for i in parameters:
            if i == '*':
                parameters.remove(i)
        return parameters

#é’ˆå¯¹tfå‡½æ•°æ–‡æ¡£è¿›è¡Œå¤„ç†
def extract_parameters_tf(api_doc, api_def):
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…Argséƒ¨åˆ†çš„æ‰€æœ‰å‚æ•°
    #tfâ†“
    #pattern = r'Args:\n(.*?)(?=\n\n|\n\w+:|$)'
    #torchâ†“
    if "Args:" in api_doc:
        pattern = r'Args:\n(.*?)(?=\n\w+:|Returns:|$)'
        args_section = re.search(pattern, api_doc, re.DOTALL)
        
        if not args_section:
            return []
        
        # æå–æ¯ä¸ªå‚æ•°è¡Œ
        param_lines = args_section.group(1).split('\n')
        #for i in param_lines:
            #print(i)

        parameters = []
        
        for line in param_lines:
            # åŒ¹é…å‚æ•°åï¼ˆç¬¬ä¸€ä¸ªå†’å·å‰çš„å•è¯ï¼‰
            param_match = re.match(r'^\s*(\w+)\s*:', line.strip())
            if param_match:
                parameters.append(param_match.group(1))
        
        return parameters
    else:
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°Argséƒ¨åˆ†ï¼Œä½¿ç”¨api_defè·å–å‚æ•°åˆ—è¡¨
        param_str = api_def.split('(')[1].split(')')[0]
        parameters = [p.strip().split('=')[0] for p in param_str.split(',')]
        for i in parameters:
            if i == '*':
                parameters.remove(i)
        return parameters

#è·å–å‡½æ•°æ‰€æœ‰åˆæ³•å‚æ•°
def get_all_parameters(api_name: str):
    json_filename = f"{lib_name}_conditions.json"

    current_dir = os.path.dirname(os.path.abspath(__file__))
    json_path = os.path.join(current_dir, "conditions", json_filename)
    
    with open(json_path, 'r', encoding='utf-8') as file:
        data = json.load(file)
    
    if api_name not in data:
        raise KeyError(f"API '{api_name}' not found in JSON file")
    
    if "Parameter type" not in data[api_name]:
        return []
    
    return list(data[api_name]["Parameter type"].keys())
    # api_doc = get_doc(fun_string)
    # å…ˆæ ¹æ®api_docè·å–å‚æ•°åˆ—è¡¨
    # å¦‚æœä¸èƒ½é€šè¿‡api_docè·å–å‚æ•°åˆ—è¡¨ï¼Œåˆ™ä½¿ç”¨api_defè·å–å‚æ•°åˆ—è¡¨

    # if lib_name == "torch":
    #     return extract_parameters_torch(api_doc, api_def)
    # elif lib_name == "tf":
    #     return extract_parameters_tf(api_doc, api_def)
    # é€‰æ‹©å¯¹åº”çš„å‚æ•°åˆ—è¡¨æå–æ–¹æ³•æå–å‚æ•°å‚æ•°åˆ—è¡¨


#è·å–æ‰€æœ‰å‚æ•°çš„ç»„åˆ
def generate_all_combinations(args):
    all_combinations = []
    for r in range(1, len(args) + 1):
        combinations = itertools.combinations(args, r)
        all_combinations.extend([list(comb) for comb in combinations])
    return all_combinations

#è¿‡æ»¤ä¸åˆæ³•çš„å‚æ•°ç»„åˆ
def filter_combinations(combinations, conditions):
 
    # è·å–æ¡ä»¶
    mandatory_params = conditions.get('Mandatory Parameters', [])
    exclusive_groups = conditions.get('Mutually Exclusive Parameter Pairs', [])
    coexistence_groups = conditions.get('Mandatory Coexistence Parameters', [])

    filtered = []
    
    for combo in combinations:
        # 1. æ£€æŸ¥æ˜¯å¦åŒ…å«æ‰€æœ‰å¿…é¡»å‚æ•°
        if mandatory_params:
            if mandatory_params and not all(param in combo for param in mandatory_params):
                continue

            
        # 2. æ£€æŸ¥æ˜¯å¦ä¸åŒ…å«ä»»ä½•äº’æ–¥å‚æ•°ç»„ä¸­çš„å…¨éƒ¨å‚æ•°
        def filter_exclusive_combinations(param_combinations, exclusive_pairs):
            param_set = set(param_combinations)
            for pair in exclusive_pairs:
                if all(p in param_set for p in pair):
                    return False
            return True
        if not filter_exclusive_combinations(combo, exclusive_groups):
            continue

        # 3. æ£€æŸ¥æ˜¯å¦æ»¡è¶³æ‰€æœ‰å¿…é¡»å…±å­˜çš„å‚æ•°ç»„
        # å¯¹äºæ¯ä¸ªå…±å­˜ç»„ï¼Œæ£€æŸ¥ç»„åˆä¸­æ˜¯å¦è‡³å°‘åŒ…å«è¯¥ç»„ä¸­çš„ä¸€ä¸ªå‚æ•°
        # å¦‚æœå…±å­˜ç»„ä¸ºç©ºï¼Œåˆ™è·³è¿‡æ­¤æ£€æŸ¥
        meets_coexistence = all(
            all(param in combo for param in group)
            for group in coexistence_groups
            )
        
        if not meets_coexistence:
            continue
        
        filtered.append(combo)
    
    return filtered

#è¯»å–æ–‡ä»¶
def read_file(file_path):
    api_names = []

    with open(file_path, 'r', encoding='utf-8') as file:
        lines = [line.strip() for line in file]
    for i in lines:
        pattern = r"^[^(]*"
        match = re.match(pattern, i)
        api_names.append(match.group() if match else None)

    return api_names

# å‘JSONæ–‡ä»¶ä¸­æ·»åŠ APIæ¡ä»¶
def append_api_condition_to_json(path, fun_string, new_data):
    if not new_data:
        condition_dict = {}
    else:
        try:
            # æŠŠå­—ç¬¦ä¸²è§£æä¸º Python å­—å…¸
            condition_dict = json.loads(new_data)
        except json.JSONDecodeError as e:
            add_log(f"JSON è§£æé”™è¯¯: {e}")
            return

    # è¯»å–åŸå§‹ JSON æ–‡ä»¶å†…å®¹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if os.path.exists(path):
        try:
            with open(path, 'r', encoding='utf-8') as f:
                data = json.load(f)
        except json.JSONDecodeError:
            data = {}
    else:
        data = {}

    # æ·»åŠ æˆ–æ›´æ–°é¡¹
    data[fun_string] = condition_dict

    # å†™å› JSON æ–‡ä»¶
    with open(path, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=4, ensure_ascii=False)

# è·å–JSONæ–‡ä»¶ä¸­çš„api_conditions
def get_api_conditions(fun_string, file_path):
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        # ç›´æ¥è·å–æŒ‡å®šå‡½æ•°åå¯¹åº”çš„æ¡ä»¶å­—å…¸
        return data.get(fun_string, None)

    except (FileNotFoundError, json.JSONDecodeError) as e:
        add_log(f"Error reading file: {e}")
        return None

# è®°å½•log
def add_log(path, log):
    #with open(f'/tmp/Momo_test/{lib_name}_log.txt', "a", encoding="utf-8") as f:
    # with open(r'C:\Users\86184\Desktop\torch_log.txt', "a", encoding="utf-8") as f:
    file_path = path
    
    # ç¡®ä¿ç›®å½•å’Œæ–‡ä»¶éƒ½å­˜åœ¨
    os.makedirs(os.path.dirname(file_path), exist_ok=True)
    
    # å†™å…¥æ—¥å¿—ï¼ˆå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ä¼šè‡ªåŠ¨åˆ›å»ºï¼‰
    with open(file_path, "a", encoding="utf-8") as f:
        print(log)  # æ‰“å°åˆ°æ§åˆ¶å°
        print(log, file=f)  # å†™å…¥æ–‡ä»¶

# è®°å½•log
def local_add_log(log):
    # with open(f'/tmp/Momo_test/{lib_name}_log.txt', "a", encoding="utf-8") as f:
    with open(f'C:/Users/86184/Desktop/local_{lib_name}_filter_log.txt', "a", encoding="utf-8") as f:
        print(log)  # æ‰“å°åˆ°æ§åˆ¶å°
        print(log, file=f)  # å†™å…¥æ–‡ä»¶

# å°†è¿‡æ»¤å¥½çš„å‚æ•°ç»„åˆå†™å…¥JSONæ–‡ä»¶
def append_filtered_combinations_to_json(path, fun_string, new_data):
    
    # å¦‚æœæ–‡ä»¶å­˜åœ¨ï¼ŒåŠ è½½å†…å®¹ï¼›å¦åˆ™åˆ›å»ºç©ºå­—å…¸
    if os.path.exists(path):
        with open(path, 'r', encoding='utf-8') as f:
            try:
                data = json.load(f)
            except json.JSONDecodeError:
                data = {}
    else:
        data = {}

    # æ›´æ–°æˆ–æ·»åŠ æ–°æ•°æ®
    data[fun_string] = new_data

    # å†™å…¥åˆ°æ–‡ä»¶
    with open(path, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=4, ensure_ascii=False)

# è¯»å–JSONæ–‡ä»¶ä¸­çš„è¿‡æ»¤å¥½çš„å‚æ•°ç»„åˆ
def is_file_too_large(file_path, max_size_mb=10):
    """
    æ£€æŸ¥æ–‡ä»¶æ˜¯å¦è¿‡å¤§
    
    å‚æ•°:
    file_path (str): æ–‡ä»¶è·¯å¾„
    max_size_mb (float): æœ€å¤§å…è®¸çš„æ–‡ä»¶å¤§å°ï¼ˆMBï¼‰ï¼Œé»˜è®¤10MB
    
    è¿”å›:
    bool: å¦‚æœæ–‡ä»¶è¶…è¿‡æŒ‡å®šå¤§å°è¿”å›Trueï¼Œå¦åˆ™è¿”å›False
    """
    try:
        if not os.path.exists(file_path):
            return False
            
        file_size = os.path.getsize(file_path)  # å­—èŠ‚æ•°
        file_size_mb = file_size / (1024 * 1024)  # è½¬æ¢ä¸ºMB
        
        return file_size_mb > max_size_mb
        
    except Exception as e:
        print(f"æ£€æŸ¥æ–‡ä»¶å¤§å°æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")
        return False


# æ‰‹åŠ¨å¤„ç†output
def handle_output(text: str, model_path: str):
    if model_path == "/nasdata/haoyahui/Model/Meta-Llama-3-70B-Instruct":
        target = "  6.Notions:\n    Only output the json content of the example in the output format, do not add explanations.assistant\n"
        start_index = text.find(target) + len(target)
        json_content = text[start_index:].strip()
        try:
            return json_content
        except json.JSONDecodeError as e:
            return None
    if "DeepSeek-R1-Distill-Qwen-32B" in model_path:
        end_tag = "</think>"
        if end_tag not in text:
            add_log("æœªæ‰¾åˆ° </think> æ ‡ç­¾")
            return None

        # è·å– </think> åçš„å†…å®¹
        after_think = text.split(end_tag, 1)[1].strip()
        for i in range(len(after_think)-1, -1, -1):
            if after_think[i] == '}':
                # æ‰¾åˆ°æœ€åä¸€ä¸ª'}'ï¼Œè¿”å›ä»å¼€å¤´åˆ°è¯¥ä½ç½®çš„å­ä¸²
                return after_think[:i+1]

        try:
            return after_think
        except json.JSONDecodeError as e:
            return None


def extract_clean_json(text: str):
    """
    ä»å¤§æ¨¡å‹è¾“å‡ºä¸­æŠ½å– </think> åçš„ JSONï¼Œ
    è‡ªåŠ¨è¡¥å¤§æ‹¬å·ã€å»é™¤é‡å¤å­—æ®µã€ä¿®å¤å¸¸è§é”™è¯¯ï¼Œè¿”å›æœ€ç»ˆè§£æå‡ºçš„ dictã€‚
    """

    end_tag = "</think>"
    if end_tag not in text:
        return None

    # 1. è·å– </think> åçš„å†…å®¹
    after = text.split(end_tag, 1)[1].strip()

    # 2. å®šä½ JSON å¼€å§‹ä½ç½®
    start = after.find("{")
    if start == -1:
        return None

    fragment = after[start:]

    # 3. ä½¿ç”¨å¤§æ‹¬å·å¹³è¡¡æå–å®Œæ•´ JSON å­—ç¬¦ä¸²
    json_str = balance_json_braces(fragment)

    # 4. å¼ºåˆ¶å»æ‰æœ«å°¾é JSON å†…å®¹
    json_str = trim_after_last_brace(json_str)

    # 5. å°è¯•è§£æ JSON
    try:
        data = json.loads(json_str)
    except Exception:
        # å¦‚æœè§£æå¤±è´¥ï¼Œå°è¯•å¢å¼ºä¿®å¤
        fixed = force_fix_json(json_str)
        try:
            data = json.loads(fixed)
        except Exception:
            return None

    # 6. constraints å»é‡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if isinstance(data, dict) and "constraints" in data:
        data["constraints"] = list(dict.fromkeys(data["constraints"]))

    return data


def balance_json_braces(fragment: str) -> str:
    """
    ä½¿ç”¨å¤§æ‹¬å·å¹³è¡¡ç®—æ³•æå–æœ€æ—©é—­åˆçš„ JSONã€‚
    å¦‚æœç¼ºå¤± '}' åˆ™è‡ªåŠ¨è¡¥é½ã€‚
    """
    balance = 0
    end_index = -1

    for i, ch in enumerate(fragment):
        if ch == "{":
            balance += 1
        elif ch == "}":
            balance -= 1

        # æ‰¾åˆ°å®Œæ•´å¹³è¡¡ç‚¹
        if balance == 0 and i > 0:
            end_index = i
            break

    # å¦‚æœæ²¡é—­åˆ â†’ è‡ªåŠ¨è¡¥é½ç¼ºå¤±æ‹¬å·
    if end_index == -1:
        return fragment + "}" * balance
    else:
        return fragment[:end_index + 1]


def trim_after_last_brace(s: str) -> str:
    """
    å»æ‰ JSON åçš„å¤šä½™æ–‡æœ¬ï¼Œåªä¿ç•™åˆ°æœ€åä¸€ä¸ªå¤§æ‹¬å·ã€‚
    """
    last = s.rfind("}")
    if last != -1:
        return s[:last + 1]
    return s


def force_fix_json(s: str) -> str:
    """
    å¼ºåˆ¶ä¿®å¤ JSONï¼šç”¨äº json.loads() åˆæ¬¡å¤±è´¥çš„æƒ…å†µã€‚
    ç›®å‰ä¸»è¦æ“ä½œï¼š
    - å»æ‰ JSON åå¤šä½™éƒ¨åˆ†
    - è¡¥é½ç¼ºå¤±æ‹¬å·
    """
    s = trim_after_last_brace(s)

    # ç®€å•æ£€æŸ¥å¤§æ‹¬å·å¹³è¡¡ï¼Œå¦‚æœä¸å¤Ÿè¡¥é½
    open_count = s.count("{")
    close_count = s.count("}")

    if close_count < open_count:
        s += "}" * (open_count - close_count)

    return s



# å°è£…ä¸åŒæ¨¡å‹çš„è¾“å…¥è¾“å‡ºæ¨¡å¼ 
def generate_input(prompt, tokenizer, model):

    # model_path_list = [
    #     "/nasdata/haoyahui/Model/Meta-Llama-3-70B-Instruct",
    #     "/nasdata/haoyahui/Model/DeepSeek-R1-Distill-Qwen-32B"
    #     "/home/chaoni/haoyahui/Model/DeepSeek-R1-Distill-Qwen-32B"
    # ]

    # if model_path not in model_path_list:
    
    #     inputs = tokenizer(
    #         prompt,
    #         return_tensors="pt",
    #         truncation=True,
    #         max_length=4096,
    #         padding=True
    #     )
    # else:
    # print(111111111111111111)
    inputs = tokenizer.apply_chat_template(
        prompt,
        return_tensors="pt",
        truncation=True,
        max_length=4096,
        #padding = "max_length"
        padding=True
    )
    return inputs

def generate_output(inputs, model, tokenizer):
    # model_path_list = [
    #     "/nasdata/haoyahui/Model/Meta-Llama-3-70B-Instruct",
    #     "/nasdata/haoyahui/Model/DeepSeek-R1-Distill-Qwen-32B"
    # ]

    # if model_path not in model_path_list:
    #     outputs = model.generate(
    #         **inputs,
    #         max_new_tokens=2048,  # å¯ä»¥æ›´å¤§
    #         do_sample=False,      # å¯ç”¨é‡‡æ ·
    #         temperature=1.0,     # å¢åŠ å¤šæ ·æ€§
    #         top_p=1.0,
    #         eos_token_id=tokenizer.eos_token_id,
    #         pad_token_id=tokenizer.pad_token_id
    #     )
    # else:
    outputs = model.generate(
        inputs,
        max_new_tokens=2048,  # å¯ä»¥æ›´å¤§
        do_sample=False,      # å¯ç”¨é‡‡æ ·
        temperature=1.0,     # å¢åŠ å¤šæ ·æ€§
        top_p=1.0,
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.pad_token_id
    )
    return outputs

# é¢„é˜²åŒåå‡½æ•°
def filter_samenames(i ,fun_string, api_names):
    if lib_name == "torch":
        if fun_string in torch_samename_list:
            if api_names[i+3] == fun_string:
                function_name = fun_string + "_" + str(4)
            elif api_names[i+2] == fun_string:
                function_name = fun_string + "_" + str(3)
            elif api_names[i+1] == fun_string:
                function_name = fun_string + "_" + str(1)
            else:
                function_name = fun_string+ "_" + str(2)
        else:
            function_name = fun_string
    else:
        function_name = fun_string
    return function_name


def get_all_combinations_from_json(api_name, j):
    # path = f'C:/Users/86184/Desktop/torch_combinations.json'
    k = j
    while True:
        try:
        # è¯»å–JSONæ–‡ä»¶
            with open(f'/nasdata/haoyahui/Arg_combinations/{lib_name}_combinations_{k}.json', 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # æå–api_nameé¡¹
            args_combinations = data.get(api_name)
            
        except KeyError:
            return False

        if args_combinations == None:
            k += 1
            continue
        else:
            return args_combinations, k

# è¿‡æ»¤é”™è¯¯ç»„åˆæ—¶æ–­ç‚¹ç»­ç”Ÿæˆ
def extract_invalid_parameter_combinations():
    #file_path = r'C:\Users\86184\Desktop\test.txt'
    file_path = f'/tmp/Momo_test/error_combinations/{lib_name}_log.txt'
    pattern = r"tf\.keras\.optimizers\.Ftrl çš„å‚æ•°ç»„åˆ (.*?) å¯èƒ½ä¸åˆæ³•"

    result = []
    
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            content = file.read()
            
            # ä½¿ç”¨finditeræŸ¥æ‰¾æ‰€æœ‰åŒ¹é…é¡¹
            for match in re.finditer(pattern, content, re.DOTALL):
                # æå–å‚æ•°ç»„åˆéƒ¨åˆ†
                params_str = match.group(1)
                array = eval(params_str)
                result.append(array)

    except FileNotFoundError:
        print(f"é”™è¯¯ï¼šæ–‡ä»¶ {file_path} æœªæ‰¾åˆ°")
    except Exception as e:
        print(f"è¯»å–æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")
    
    return result


#-------------------------------------
# ç»Ÿä¸€è¯»å–jsonæ¥å£
#-------------------------------------
def read_json_api(api_name, file_path, read_mode):
    if read_mode == "combination":
        j = 0
        path = file_path+f'{lib_name}_combinations_{j}.json'
        while True:
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
            if api_name in data:
                return data[api_name]  # äºŒç»´æ•°ç»„
            else:
                j += 1
                new_path = file_path+f'{lib_name}_combinations_{j}.json'
                with open(new_path, "r", encoding="utf-8") as f:
                    new_data = json.load(f)
                return new_data[api_name]
            if j > 20:
                break

    elif read_mode == "error_combinations":
        path = file_path+f'error_{lib_name}_combinations.json'
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
        if api_name in data:
            return data[api_name] 

    elif read_mode == "arg_space":
        path = file_path+f'{lib_name}_arg_space.json'
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
        if api_name in data:
            return data[api_name]

    elif read_mode == "src_code":
        path = file_path+f'{lib_name}_api_sources.json'
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
        if api_name in data:
            return data[api_name] 
    elif read_mode == "conditions":
        path = file_path+f'{lib_name}_conditions.json'
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
        if api_name in data:
            return data[api_name] 
    else:
        return None

# =========================================
# ä¿å­˜ API è¾“å…¥ä¿¡æ¯çš„å·¥å…·å‡½æ•°
# =========================================
def save_api_inputs(api_name, api_inputs, save_path):
    """
    å°† {api_name: api_inputs} å¢é‡å†™å…¥ JSON æ–‡ä»¶ã€‚
    å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨åˆ™åˆ›å»ºï¼Œå­˜åœ¨åˆ™åœ¨åŸå†…å®¹ä¸Šè¿½åŠ ã€‚
    """
    # 1ï¸âƒ£ å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ â†’ åˆ›å»ºç›®å½• & ç©ºæ–‡ä»¶
    if not os.path.exists(save_path):
        dir_path = os.path.dirname(save_path)
        if dir_path and not os.path.exists(dir_path):
            os.makedirs(dir_path, exist_ok=True)
        with open(save_path, "w", encoding="utf-8") as f:
            json.dump({}, f, indent=4, ensure_ascii=False)
        print(f"[ğŸ“ Created] æ–°æ–‡ä»¶å·²åˆ›å»º: {save_path}")

    # 2ï¸âƒ£ è¯»å–å·²æœ‰æ•°æ®
    with open(save_path, "r", encoding="utf-8") as f:
        try:
            all_data = json.load(f)
        except json.JSONDecodeError:
            all_data = {}

    # 3ï¸âƒ£ åˆå¹¶ï¼ˆå¢é‡ä¿å­˜ï¼‰
    if api_name in all_data:
        all_data[api_name].extend(api_inputs)
    else:
        all_data[api_name] = api_inputs

    # 4ï¸âƒ£ å†™å›æ–‡ä»¶
    with open(save_path, "w", encoding="utf-8") as f:
        json.dump(all_data, f, indent=4, ensure_ascii=False)

# =========================================
# æ ¹æ®è§„èŒƒåŒ–çš„apiè¾¹ç•Œç”Ÿæˆæµ‹è¯•è¾“å…¥çš„ç®¡é“
# =========================================

# ç”Ÿæˆå¤æ‚å‚æ•°
def generate_complex_param(api_name, param_name, param_info, constraints, model, tokenizer):
    """
    ä½¿ç”¨ LLM ç”Ÿæˆå¤æ‚å¯¹è±¡
    """
    api_doc = get_doc(api_name)
    prompt = generate_prompt_4(api_name, param_name, param_info, constraints, api_doc)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token  
    inputs = generate_input(prompt, tokenizer, model)

    # æŠŠinputsæ”¾åˆ°æ¨¡å‹å‚æ•°æ‰€åœ¨è®¾å¤‡
    inputs = inputs.to(next(model.parameters()).device)

    outputs = generate_output(inputs, model, tokenizer)
    # è§£ç è¾“å‡º
    outputs_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    complex_input = handle_output(outputs_text, model_path)

    return complex_input["test_values"]

def generate_tensor_param_cases(param_name, param_info, max_dim_limit=256):
    """
    ä¸º Tensor ç±»å‹å‚æ•°ç”Ÿæˆä¸€ç³»åˆ—è¦†ç›–æ€§æµ‹è¯•æ ·æœ¬å­—ç¬¦ä¸²ã€‚
    âœ… ç‰¹æ€§ï¼š
      - è‡ªåŠ¨ç”Ÿæˆ min / mid / max çš„ç»´åº¦ç»„åˆ
      - æ”¯æŒ floatã€intã€boolã€complexã€bfloat16 ç­‰ dtype
      - è‡ªåŠ¨é˜²æ­¢è¶…å¤§å½¢çŠ¶
      - ç»Ÿä¸€ dtype è§£æï¼Œä¸å†ç”Ÿæˆ torch.torch.float128 ä¹‹ç±»çš„é”™è¯¯
    """
    shape_min = param_info.get("shape_min", [1])
    shape_max = param_info.get("shape_max", [3])
    dtypes = param_info.get("dtypes", ["float32"])

    # -------------------------------
    # 1ï¸âƒ£ shape è¾¹ç•Œç»„åˆï¼šå– min / mid / max
    # -------------------------------
    shape_cases = []
    for lo, hi in zip(shape_min, shape_max):
        lo = min(int(lo), max_dim_limit)
        hi = min(int(hi), max_dim_limit)

        if lo == hi:
            shape_cases.append([lo])
        else:
            mid = (lo + hi) // 2
            mid = min(mid, max_dim_limit)
            shape_cases.append([lo, mid, hi])

    # æ‰€æœ‰ç»„åˆ
    shape_combos = list(itertools.product(*shape_cases))

    # -------------------------------
    # 2ï¸âƒ£ dtype Ã— shape ç»„åˆç”Ÿæˆå­—ç¬¦ä¸²
    # -------------------------------
    samples = []
    for dtype_str in dtypes:
        # æ¸…ç† dtype åç§°ï¼ˆå¯èƒ½æ˜¯ "torch.float32"ï¼‰
        clean_dtype = dtype_str.split(".")[-1].strip()
        if not hasattr(torch, clean_dtype):
            # é¿å…å‡ºç° float128 / ä¼ªç±»å‹
            clean_dtype = "float32"

        for shape in shape_combos:
            shape_str = ", ".join(str(s) for s in shape)

            # æ ¹æ® dtype æ„é€ è¡¨è¾¾å¼
            if any(k in clean_dtype for k in ["float", "half", "bfloat"]):
                expr = f"{param_name} = torch.randn(({shape_str},), dtype=torch.{clean_dtype})"

            elif any(k in clean_dtype for k in ["int", "long"]):
                expr = f"{param_name} = torch.randint(0, 10, ({shape_str},), dtype=torch.{clean_dtype})"

            elif "uint8" in clean_dtype:
                expr = f"{param_name} = torch.randint(0, 256, ({shape_str},), dtype=torch.uint8)"

            elif "bool" in clean_dtype:
                expr = f"{param_name} = (torch.rand(({shape_str},)) > 0.5).to(dtype=torch.bool)"

            elif "complex" in clean_dtype:
                # å¯¹ complex ç±»å‹ï¼Œåº•å±‚å®éƒ¨ dtype æ˜ å°„
                base = "float32" if clean_dtype == "complex64" else "float64"
                expr = (
                    f"{param_name} = (torch.randn(({shape_str},), dtype=torch.{base}) + "
                    f"1j * torch.randn(({shape_str},), dtype=torch.{base})).to(dtype=torch.{clean_dtype})"
                )

            else:
                expr = f"# Unsupported dtype: {clean_dtype}"

            samples.append(expr)

    return samples

def generate_scalar_param_cases(param_name, param_info):
    p_type = param_info.get("type")
    lo = param_info.get("min", 0)
    hi = param_info.get("max", 10)

    # 1ï¸âƒ£ è®¡ç®—ä¸­é—´ç‚¹
    mid = (lo + hi) / 2

    # 2ï¸âƒ£ ç”ŸæˆåŸºæœ¬å–å€¼
    if p_type == "int":
        # ç¡®ä¿æ•´æ•°èŒƒå›´å†…ä¸é‡å¤
        values = sorted(set([lo, int(mid), hi]))
        samples = [f"{param_name}={v}" for v in values]
    elif p_type == "float":
        # åŒ…æ‹¬æœ€å°ã€æœ€å¤§ã€ä¸­é—´ã€è¾¹ç•Œåç§»
        mid_lo = lo + (mid - lo) / 2
        mid_hi = mid + (hi - mid) / 2
        values = [lo, mid_lo, mid, mid_hi, hi]
        samples = [f"{param_name}={round(v, 6)}" for v in values]
    else:
        raise ValueError(f"Unsupported type: {p_type}")

    return samples

# ç”Ÿæˆç®€å•å‚æ•°
def generate_sample_param(api_name, param, param_info):
    """
    æ ¹æ® param_info çš„ç±»å‹ç”Ÿæˆå•ä¸ªå‚æ•°æ ·æœ¬ã€‚
    æ”¯æŒ Tensorã€intã€floatã€boolã€strã€optionalã€choices ç­‰ã€‚
    """
    p_type = param_info.get("type")

    # 1ï¸âƒ£ Tensor ç±»å‹
    if "Tensor" in p_type:

        return generate_tensor_param_cases(param, param_info)

    # 2ï¸âƒ£ æ•°å€¼å‹å‚æ•°
    elif p_type in ["Int", "Float"]:
        return generate_scalar_param_cases(param, param_info)

    # 3ï¸âƒ£å¸ƒå°”å‹å‚æ•°
    elif "Bool" in p_type:
        samples = [f"{param}=True", f"{param}=False"]
        return samples

    # 4ï¸âƒ£ å­—ç¬¦ä¸²å‚æ•°ï¼ˆæ—  choicesï¼‰
    elif "Str" in p_type and "choices" not in param_info:
        length = param_info.get("length", 5)
        samples = []
        for _ in range(2):  # ç”Ÿæˆä¸¤ä¸ªä¸åŒå­—ç¬¦ä¸²
            s = ''.join(random.choices('abcdefghijklmnopqrstuvwxyz', k=length))
            samples.append(f"{param}={s}")
        return samples

    # 5ï¸âƒ£ å¯é€‰å‚æ•°ï¼ˆå¯èƒ½ä¸º Noneï¼‰
    elif "Optional" in p_type:
        samples = []
        if "choices" in param_info:
            # åŒ…å« None + æ‰€æœ‰æšä¸¾é€‰é¡¹
            samples = [f"{param}=None"] + [f"{param}={choice}" for choice in param_info["choices"]]
        else:
            # é»˜è®¤åŒ…å« None + ä¸€ä¸ªç¤ºä¾‹å€¼
            samples = [f"{param}=None", f"{param}=some_value"]
        return samples

    # 6ï¸âƒ£ æœ‰ choicesï¼ˆæšä¸¾ï¼‰å‚æ•°
    elif "choices" in param_info:
        choices = param_info["choices"]
        samples = [f"{param}=None"] + [f"{param}={choice}" for choice in choices] 
        return samples
    else:
        raise ValueError(f"[{api_name}] Unsupported type: {p_type}")

# æ£€æŸ¥çº¦æŸæ¡ä»¶
def check_constraints(combo, constraints):


    return False

# å°†å…ƒç»„åˆ—è¡¨è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
def convert_list_to_dict_list(data_list):
    """
    data_list æ˜¯å½¢å¦‚:
        [('input = ...', 'dim=1', 'index = ...'), ...]
    è¿”å›:
        [{'input': '...', 'dim': '1', 'index': '...'}, ...]
    """
    import ast

    def parse_assignment(expr: str):
        """
        å°†å­—ç¬¦ä¸² 'key = value' è§£ææˆå­—å…¸ {key: value}
        value ä¿ç•™ä¸ºåŸå§‹è¡¨è¾¾å¼å­—ç¬¦ä¸²ï¼Œä¸æ‰§è¡Œ eval
        """
        if "=" not in expr:
            raise ValueError("è¡¨è¾¾å¼å¿…é¡»åŒ…å« '='")

        key, value = expr.split("=", 1)
        key = key.strip()
        value = value.strip()

        return {key: value}

    result = []
    for tup in data_list:
        item_dict = {}
        for expr in tup:
            # ä½¿ç”¨ parse_assignment è§£æè¡¨è¾¾å¼
            parsed = parse_assignment(expr)
            item_dict.update(parsed)
        result.append(item_dict)
    
    return result



def generate_test_inputs_from_api_boundaries(api_name, api_boundaries, model=None, tokenizer=None):
    """
    æ ¹æ® API çš„è¾¹ç•Œè§„èŒƒï¼Œç”Ÿæˆæ»¡è¶³çº¦æŸçš„æµ‹è¯•è¾“å…¥ç»„åˆã€‚
    """
    params = api_boundaries.get("params", {})
    constraints = api_boundaries.get("constraints", [])

    # 1ï¸âƒ£ ä¸ºæ¯ä¸ªå‚æ•°ç”Ÿæˆå€™é€‰æ ·æœ¬
    candidate_dict = {}
    for param_name, param_info in params.items():
        p_type = param_info.get("type")
        if p_type in ["Tensor", "int", "float", "bool", "str", "optional"]:
            candidate_dict[param_name] = generate_sample_param(api_name, param_name, param_info)
        else:
            # ä½¿ç”¨æ¨¡å‹ç”Ÿæˆå¤æ‚å‚æ•°
            candidate_dict[param_name] = [generate_complex_param(api_name, param_name, param_info, constraints, model, tokenizer)]

    # 2ï¸âƒ£ ç”Ÿæˆæ‰€æœ‰å‚æ•°çš„ç¬›å¡å°”ç§¯ç»„åˆ
    keys = list(candidate_dict.keys())
    all_combos = list(itertools.product(*[candidate_dict[k] for k in keys]))

    # 3ï¸âƒ£ çº¦æŸç­›é€‰
    valid_inputs = []
    i = 1
    length = len(all_combos)
    for combo in all_combos:
        print("ç¬¬"+str(i)+"/"+str(length)+"ä¸ª")
        i += 1
        if check_constraints(combo, constraints):

            valid_inputs.append(combo)

    # 4ï¸âƒ£ è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
    new_combos = convert_list_to_dict_list(valid_inputs)

    return new_combos




# test_bundary = {
# "params": {
# "input": {
# "type": "Tensor",
# "shape_min": [1, 1, 1],
# "shape_max": [128, 4096, 65536],
# "dtypes": ["torch.float16", "torch.bfloat16", "torch.float32", "torch.float64", "torch.complex64", "torch.complex128"]
# },
# "dim": {
# "type": "int",
# "min": 0,
# "max": 3
# },
# "index": {
# "type": "Tensor",
# "shape_min": [1],
# "shape_max": [4096],
# "dtypes": ["torch.int32", "torch.int64"]
# }
# },
# "constraints": [
# "input.dtype == index.dtype",
# "input.dim() >= 1",
# "index.shape[0] >= 1",
# "dim >= 0 and dim < input.dim()",
# "index.shape[0] == input.shape[dim]"
# ]
# }
test_bundary = {
"params": {
"input": {
"type": "Tensor",
"shape_min": [1, 1, 1],
"shape_max": [128, 4096, 65536],
"dtypes": ["torch.float16", "torch.bfloat16", "torch.float32", "torch.float64", "torch.complex64", "torch.complex128"]
},
"dim": {
"type": "int",
"min": 0,
"max": 3
},
"index": {
"type": "Tensor",
"shape_min": [1],
"shape_max": [4096],
"dtypes": ["torch.int32", "torch.int64"]
}
},
"constraints": [
"input.dtype == index.dtype",
"input.dim() >= 1",
"index.shape[0] >= 1",
"dim >= 0 and dim < input.dim()",
"index.shape[0] == input.shape[dim]"
]
}

# a = generate_test_inputs_from_api_boundaries(api_name = "1", api_boundaries = test_bundary, model=None, tokenizer=None)
# for i in a:
#     print(i)



def convert_input_to_string(params):
    """Convert all Tensors in params to torch.randn string expressions."""
    stringified = {}
    for k, v in params.items():
        if isinstance(v, torch.Tensor):
            shape = tuple(v.shape)
            dtype = str(v.dtype)
            # ç®€åŒ–è¡¨è¾¾ï¼šfloat32 â†’ é»˜è®¤ torch.randn
            if dtype == "torch.float32":
                stringified[k] = f"torch.randn{shape}"
            else:
                stringified[k] = f"torch.randn{shape}, dtype={dtype}"
        else:
            stringified[k] = v
    return stringified

def execute_api_template(run_api_func, test_inputs, log_path="error_log.json",
                         timeout_s=30, perf_time_threshold=5.0, mem_threshold_gb=8):
    """
    æ‰§è¡Œ run_api å‡½æ•°ï¼Œå¯¹è¾“å…¥è¿›è¡Œæ‰¹é‡æµ‹è¯•ã€‚
    ä»…è®°å½•å‡ºé”™æ ·ä¾‹ï¼ˆCrash / Numerical / Performanceï¼‰ã€‚
    """

    results = {
        "crash": [],
        "numerical": [],
        "performance": []
    }

    def record_issue(issue_type, input_data, err_msg):
        # è½¬æ¢è¾“å…¥ä¸ºå­—ç¬¦ä¸²è¡¨è¾¾
        safe_input = convert_input_to_string(input_data)
        results[issue_type].append({
            "input": safe_input,
            "error": err_msg
        })

    def get_memory_usage_gb():
        process = psutil.Process(os.getpid())
        return process.memory_info().rss / (1024 ** 3)

    for i, params in enumerate(test_inputs):
        torch.cuda.empty_cache()
        gc.collect()
        start_mem = get_memory_usage_gb()
        start_time = time.time()

        try:
            # æ‰§è¡Œ API
            result = run_api_func(**params)
            elapsed = time.time() - start_time
            end_mem = get_memory_usage_gb()

            # æ€§èƒ½å¼‚å¸¸
            if elapsed > perf_time_threshold or (end_mem - start_mem) > mem_threshold_gb:
                record_issue("performance", params,
                             f"Runtime {elapsed:.2f}s, MemDelta {end_mem - start_mem:.2f} GB")

            # æ•°å€¼å¼‚å¸¸
            def has_nan_or_inf(t):
                return isinstance(t, torch.Tensor) and (torch.isnan(t).any() or torch.isinf(t).any())

            if isinstance(result, torch.Tensor):
                if has_nan_or_inf(result):
                    record_issue("numerical", params, "NaN or Inf in output")
            elif isinstance(result, (tuple, list)):
                for r in result:
                    if has_nan_or_inf(r):
                        record_issue("numerical", params, "NaN or Inf in tuple output")
                        break

        except RuntimeError as e:
            err_msg = str(e)
            if "CUDA" in err_msg or "device-side assert" in err_msg or "out of memory" in err_msg:
                record_issue("crash", params, f"CUDA-related crash: {err_msg}")
            else:
                record_issue("crash", params, f"RuntimeError: {err_msg}")

        except KeyboardInterrupt:
            print("â›”ï¸ Interrupted by user.")
            break

        except Exception:
            record_issue("crash", params, traceback.format_exc())

        # è¶…æ—¶æ£€æµ‹
        elapsed = time.time() - start_time
        if elapsed > timeout_s:
            record_issue("performance", params, f"Timeout: exceeded {timeout_s}s")

    # ä¿å­˜æ—¥å¿—ï¼ˆä»…åŒ…å«æŠ¥é”™é¡¹ï¼‰
    with open(log_path, "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)

    print(f"\nâš ï¸  Error log written to {log_path}")
    for k, v in results.items():
        print(f"  {k.upper():12s}: {len(v)} cases")

    return results